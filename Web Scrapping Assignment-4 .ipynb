{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d329c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72ed852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dharmesh singh jadon\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through rows in the table (skipping the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract data from each column\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip().replace(',', '')  # Remove commas from views\n",
    "    \n",
    "    # Append data to lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}, Name: {name_list[i]}, Artist: {artist_list[i]}, Upload Date: {upload_date_list[i]}, Views: {views_list[i]}\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through rows in the table (skipping the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract data from each column\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[4].text.strip()\n",
    "    views = columns[3].text.strip().replace(',', '')  # Remove commas from views\n",
    "    \n",
    "    # Append data to lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}, Name: {name_list[i]}, Artist: {artist_list[i]}, Upload Date: {upload_date_list[i]}, Views: {views_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f262665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940dafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find('a', {'href': '/international/fixtures'})\n",
    "fixtures_url = \"https://www.bcci.tv\" + fixtures_link['href']\n",
    "\n",
    "# Send a GET request to the fixtures URL\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "# Parse the HTML content of the fixtures page\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "\n",
    "# Find the container with the fixture details\n",
    "fixture_container = fixtures_soup.find('div', {'class': 'js-list'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Iterate through fixtures\n",
    "fixtures = fixture_container.find_all('li', {'class': 'js-list-item'})\n",
    "for fixture in fixtures:\n",
    "    series = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__datetime desktop-only'}).find('span', {'class': 'fixture__date'}).text.strip()\n",
    "    time = fixture.find('div', {'class': 'fixture__datetime desktop-only'}).find('span', {'class': 'fixture__time'}).text.strip()\n",
    "\n",
    "    # Append data to lists\n",
    "    series_list.append(series)\n",
    "    place_list.append(place)\n",
    "    date_list.append(date)\n",
    "    time_list.append(time)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(series_list)):\n",
    "    print(f\"Series: {series_list[i]}, Place: {place_list[i]}, Date: {date_list[i]}, Time: {time_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666f3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d498e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_link = soup.find('a', {'title': 'Economy'})\n",
    "economy_url = economy_link['href']\n",
    "\n",
    "# Send a GET request to the economy page URL\n",
    "economy_response = requests.get(economy_url)\n",
    "\n",
    "# Parse the HTML content of the economy page\n",
    "economy_soup = BeautifulSoup(economy_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the State-wise GDP page\n",
    "state_gdp_link = economy_soup.find('a', {'href': '/economy/gdp-of-indian-states'})\n",
    "state_gdp_url = \"http://statisticstimes.com\" + state_gdp_link['href']\n",
    "\n",
    "# Send a GET request to the State-wise GDP page URL\n",
    "state_gdp_response = requests.get(state_gdp_url)\n",
    "\n",
    "# Parse the HTML content of the State-wise GDP page\n",
    "state_gdp_soup = BeautifulSoup(state_gdp_response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the information\n",
    "table = state_gdp_soup.find('table', {'class': 'display'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_18_19_list = []\n",
    "gsdp_19_20_list = []\n",
    "share_18_19_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Iterate through rows in the table (skipping the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract data from each column\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_18_19 = columns[2].text.strip()\n",
    "    gsdp_19_20 = columns[3].text.strip()\n",
    "    share_18_19 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    # Append data to lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_18_19_list.append(gsdp_18_19)\n",
    "    gsdp_19_20_list.append(gsdp_19_20)\n",
    "    share_18_19_list.append(share_18_19)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"Rank: {rank_list[i]}, State: {state_list[i]}, GSDP(18-19): {gsdp_18_19_list[i]}, GSDP(19-20): {gsdp_19_20_list[i]}, Share(18-19): {share_18_19_list[i]}, GDP($ billion): {gdp_billion_list[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c72a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://github.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the trending page\n",
    "trending_link = soup.find('a', {'href': '/trending'})\n",
    "trending_url = \"https://github.com\" + trending_link['href']\n",
    "\n",
    "# Send a GET request to the trending page URL\n",
    "trending_response = requests.get(trending_url)\n",
    "\n",
    "# Parse the HTML content of the trending page\n",
    "trending_soup = BeautifulSoup(trending_response.text, 'html.parser')\n",
    "\n",
    "# Find the containers with the repository details\n",
    "repo_containers = trending_soup.find_all('article', {'class': 'Box-row'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "title_list = []\n",
    "description_list = []\n",
    "contributors_list = []\n",
    "language_list = []\n",
    "\n",
    "# Iterate through repositories\n",
    "for repo in repo_containers:\n",
    "    title = repo.find('h1', {'class': 'h3'}).text.strip()\n",
    "    description = repo.find('p', {'class': 'col-9 color-text-secondary my-1 pr-4'}).text.strip() if repo.find('p', {'class': 'col-9 color-text-secondary my-1 pr-4'}) else None\n",
    "    contributors = repo.find('a', {'href': title_list[-1] + '/graphs/contributors'}) if title_list else None\n",
    "    contributors_count = contributors.find('span', {'class': 'text-bold color-text-primary'}) if contributors else None\n",
    "    language = repo.find('span', {'class': 'd-inline-block ml-0 mr-3'}).find_next('span').text.strip() if repo.find('span', {'class': 'd-inline-block ml-0 mr-3'}) else None\n",
    "\n",
    "    # Append data to lists\n",
    "    title_list.append(title)\n",
    "    description_list.append(description)\n",
    "    contributors_list.append(contributors_count.text if contributors_count else '0')\n",
    "    language_list.append(language if language else 'Not specified')\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(title_list)):\n",
    "    print(f\"Repository Title: {title_list[i]}\\nDescription: {description_list[i]}\\nContributors Count: {contributors_list[i]}\\nLanguage: {language_list[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31b32b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b96aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the Hot 100 page\n",
    "charts_link = soup.find('a', {'href': '/charts'})\n",
    "hot_100_link = charts_link.find_next('a', {'href': '/charts/hot-100'})\n",
    "\n",
    "# Construct the Hot 100 URL\n",
    "hot_100_url = \"https://www.billboard.com\" + hot_100_link['href']\n",
    "\n",
    "# Send a GET request to the Hot 100 URL\n",
    "hot_100_response = requests.get(hot_100_url)\n",
    "\n",
    "# Parse the HTML content of the Hot 100 page\n",
    "hot_100_soup = BeautifulSoup(hot_100_response.text, 'html.parser')\n",
    "\n",
    "# Find the containers with the song details\n",
    "song_containers = hot_100_soup.find_all('li', {'class': 'chart-list__element'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "song_name_list = []\n",
    "artist_name_list = []\n",
    "last_week_rank_list = []\n",
    "peak_rank_list = []\n",
    "weeks_on_board_list = []\n",
    "\n",
    "# Iterate through songs\n",
    "for song in song_containers:\n",
    "    song_name = song.find('span', {'class': 'chart-element__information__song'}).text.strip()\n",
    "    artist_name = song.find('span', {'class': 'chart-element__information__artist'}).text.strip()\n",
    "    last_week_rank = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--last'}).text.strip()\n",
    "    peak_rank = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--peak'}).text.strip()\n",
    "    weeks_on_board = song.find('span', {'class': 'chart-element__meta text--center color--secondary text--week'}).text.strip()\n",
    "\n",
    "    # Append data to lists\n",
    "    song_name_list.append(song_name)\n",
    "    artist_name_list.append(artist_name)\n",
    "    last_week_rank_list.append(last_week_rank)\n",
    "    peak_rank_list.append(peak_rank)\n",
    "    weeks_on_board_list.append(weeks_on_board)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(song_name_list)):\n",
    "    print(f\"Song Name: {song_name_list[i]}\\nArtist Name: {artist_name_list[i]}\\nLast Week Rank: {last_week_rank_list[i]}\\nPeak Rank: {peak_rank_list[i]}\\nWeeks on Board: {weeks_on_board_list[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c440936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd943756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find('table', {'class': 'in-article sortable'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "book_name_list = []\n",
    "author_name_list = []\n",
    "volumes_sold_list = []\n",
    "publisher_list = []\n",
    "genre_list = []\n",
    "\n",
    "# Iterate through rows in the table (skipping the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    # Extract data from each column\n",
    "    book_name = columns[0].text.strip()\n",
    "    author_name = columns[1].text.strip()\n",
    "    volumes_sold = columns[2].text.strip()\n",
    "    publisher = columns[3].text.strip()\n",
    "    genre = columns[4].text.strip()\n",
    "\n",
    "    # Append data to lists\n",
    "    book_name_list.append(book_name)\n",
    "    author_name_list.append(author_name)\n",
    "    volumes_sold_list.append(volumes_sold)\n",
    "    publisher_list.append(publisher)\n",
    "    genre_list.append(genre)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(book_name_list)):\n",
    "    print(f\"Book Name: {book_name_list[i]}\\nAuthor Name: {author_name_list[i]}\\nVolumes Sold: {volumes_sold_list[i]}\\nPublisher: {publisher_list[i]}\\nGenre: {genre_list[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bbc1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the containers with the TV series details\n",
    "series_containers = soup.find_all('div', {'class': 'lister-item-content'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "name_list = []\n",
    "year_span_list = []\n",
    "genre_list = []\n",
    "run_time_list = []\n",
    "ratings_list = []\n",
    "votes_list = []\n",
    "\n",
    "# Iterate through TV series\n",
    "for series in series_containers:\n",
    "    name = series.find('h3', {'class': 'lister-item-header'}).find('a').text.strip()\n",
    "    year_span = series.find('span', {'class': 'lister-item-year'}).text.strip('()')\n",
    "    genre = series.find('span', {'class': 'genre'}).text.strip()\n",
    "    run_time = series.find('span', {'class': 'runtime'}).text.strip()\n",
    "    ratings = series.find('span', {'class': 'ipl-rating-star__rating'}).text.strip()\n",
    "    votes = series.find('span', {'name': 'nv'}).text.strip().replace(',', '')\n",
    "\n",
    "    # Append data to lists\n",
    "    name_list.append(name)\n",
    "    year_span_list.append(year_span)\n",
    "    genre_list.append(genre)\n",
    "    run_time_list.append(run_time)\n",
    "    ratings_list.append(ratings)\n",
    "    votes_list.append(votes)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(name_list)):\n",
    "    print(f\"Name: {name_list[i]}\\nYear Span: {year_span_list[i]}\\nGenre: {genre_list[i]}\\nRun Time: {run_time_list[i]}\\nRatings: {ratings_list[i]}\\nVotes: {votes_list[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4278c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the Show All Datasets page\n",
    "show_all_datasets_link = soup.find('a', {'href': '/ml/datasets.php'})\n",
    "show_all_datasets_url = \"https://archive.ics.uci.edu/ml/datasets.php\"\n",
    "\n",
    "# Send a GET request to the Show All Datasets page URL\n",
    "show_all_datasets_response = requests.get(show_all_datasets_url)\n",
    "\n",
    "# Parse the HTML content of the Show All Datasets page\n",
    "show_all_datasets_soup = BeautifulSoup(show_all_datasets_response.text, 'html.parser')\n",
    "\n",
    "# Find the containers with the dataset details\n",
    "dataset_containers = show_all_datasets_soup.find_all('p', {'class': 'normal'})\n",
    "\n",
    "# Initialize lists to store the data\n",
    "dataset_name_list = []\n",
    "data_type_list = []\n",
    "task_list = []\n",
    "attribute_type_list = []\n",
    "no_of_instances_list = []\n",
    "no_of_attributes_list = []\n",
    "year_list = []\n",
    "\n",
    "# Iterate through datasets\n",
    "for dataset in dataset_containers:\n",
    "    dataset_info = dataset.text.strip().split(':')\n",
    "\n",
    "    # Extract data from each dataset\n",
    "    dataset_name = dataset_info[0]\n",
    "    data_type = dataset_info[1].split(',')[0].strip()\n",
    "    task = dataset_info[1].split(',')[1].strip()\n",
    "    attribute_type = dataset_info[2].split(',')[0].strip()\n",
    "    no_of_instances = dataset_info[2].split(',')[1].strip()\n",
    "    no_of_attributes = dataset_info[2].split(',')[2].strip()\n",
    "    year = dataset_info[3].strip()\n",
    "\n",
    "    # Append data to lists\n",
    "    dataset_name_list.append(dataset_name)\n",
    "    data_type_list.append(data_type)\n",
    "    task_list.append(task)\n",
    "    attribute_type_list.append(attribute_type)\n",
    "    no_of_instances_list.append(no_of_instances)\n",
    "    no_of_attributes_list.append(no_of_attributes)\n",
    "    year_list.append(year)\n",
    "\n",
    "# Print or use the data as needed\n",
    "for i in range(len(dataset_name_list)):\n",
    "    print(f\"Dataset Name: {dataset_name_list[i]}\\nData Type: {data_type_list[i]}\\nTask: {task_list[i]}\\nAttribute Type: {attribute_type_list[i]}\\nNo of Instances: {no_of_instances_list[i]}\\nNo of Attributes: {no_of_attributes_list[i]}\\nYear: {year_list[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25fae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8dbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53b9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b3343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b299423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c29f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
