{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52114c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "        for product in products:\n",
    "            product_name = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "            product_price = product.find('span', {'class': 'a-price'}).find('span', {'class': 'a-offscreen'}).text.strip()\n",
    "\n",
    "            print(\"Product:\", product_name)\n",
    "            print(\"Price:\", product_price)\n",
    "            print(\"------\")\n",
    "    else:\n",
    "        print(\"Failed to fetch the data.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76118b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_product_details(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        brand_name = soup.find('a', {'id': 'bylineInfo'}).text.strip() if soup.find('a', {'id': 'bylineInfo'}) else \"-\"\n",
    "        product_name = soup.find('span', {'id': 'productTitle'}).text.strip() if soup.find('span', {'id': 'productTitle'}) else \"-\"\n",
    "        price = soup.find('span', {'id': 'priceblock_ourprice'}).text.strip() if soup.find('span', {'id': 'priceblock_ourprice'}) else \"-\"\n",
    "        return_exchange = soup.find(text='Return Policy:').find_next('span').text.strip() if soup.find(text='Return Policy:') else \"-\"\n",
    "        expected_delivery = soup.find(text='Arrives:').find_next('span').text.strip() if soup.find(text='Arrives:') else \"-\"\n",
    "        availability = soup.find('div', {'id': 'availability'}).text.strip() if soup.find('div', {'id': 'availability'}) else \"-\"\n",
    "        \n",
    "        return {\n",
    "            \"Brand Name\": brand_name,\n",
    "            \"Name of the Product\": product_name,\n",
    "            \"Price\": price,\n",
    "            \"Return/Exchange\": return_exchange,\n",
    "            \"Expected Delivery\": expected_delivery,\n",
    "            \"Availability\": availability,\n",
    "            \"Product URL\": product_url\n",
    "        }\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def search_amazon(product):\n",
    "    base_url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    products_data = []\n",
    "\n",
    "    for page in range(1, 4):  # Fetch first 3 pages or available pages\n",
    "        url = f\"{base_url}&page={page}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "            \n",
    "            for product in products:\n",
    "                product_link = product.find('a', {'class': 'a-link-normal a-text-normal'})['href']\n",
    "                product_url = f\"https://www.amazon.in{product_link}\"\n",
    "                product_details = get_product_details(product_url)\n",
    "                products_data.append(product_details)\n",
    "\n",
    "    df = pd.DataFrame(products_data)\n",
    "    df.to_csv(f\"{product}_details.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_amazon(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156af776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-3\n",
    "\n",
    "pip install selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def fetch_images(search_query, num_images):\n",
    "    driver = webdriver.Chrome(executable_path='path_to_chromedriver')  # Set the path to your chromedriver\n",
    "    driver.get('https://www.google.com/imghp?hl=en')\n",
    "\n",
    "    search_bar = driver.find_element_by_name('q')\n",
    "    search_bar.send_keys(search_query)\n",
    "    search_bar.submit()\n",
    "\n",
    "    # Scroll to load more images (change the range to load more images if needed)\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Get image elements\n",
    "    images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    image_urls = set()\n",
    "\n",
    "    for image in images:\n",
    "        image.click()\n",
    "        time.sleep(2)\n",
    "        original_images = driver.find_elements_by_xpath('//img[@class=\"n3VNCb\"]')\n",
    "        for img in original_images:\n",
    "            if img.get_attribute('src') and 'http' in img.get_attribute('src'):\n",
    "                image_urls.add(img.get_attribute('src'))\n",
    "            if len(image_urls) >= num_images:\n",
    "                break\n",
    "        if len(image_urls) >= num_images:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Saving images to a folder\n",
    "    folder_name = f\"{search_query}_images\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    for i, url in enumerate(image_urls):\n",
    "        response = requests.get(url)\n",
    "        file = open(f\"{folder_name}/image_{i + 1}.jpg\", \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Fetch 10 images for each keyword\n",
    "for keyword in keywords:\n",
    "    fetch_images(keyword, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4202a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(product):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "        smartphone_data = []\n",
    "        for product in products:\n",
    "            try:\n",
    "                product_url = \"https://www.flipkart.com\" + product.find('a', {'class': 'IRpwTa'})['href']\n",
    "                brand_name = product.find('div', {'class': '_4rR01T'}).text.strip()\n",
    "                smartphone_name = product.find('a', {'class': 'IRpwTa'}).text.strip()\n",
    "                price = product.find('div', {'class': '_30jeq3'}).text.strip()\n",
    "                details = product.find_all('li', {'class': 'rgWa7D'})\n",
    "\n",
    "                color = details[0].text if len(details) > 0 else \"-\"\n",
    "                ram = details[1].text if len(details) > 1 else \"-\"\n",
    "                rom = details[2].text if len(details) > 2 else \"-\"\n",
    "                primary_camera = details[3].text if len(details) > 3 else \"-\"\n",
    "                secondary_camera = details[4].text if len(details) > 4 else \"-\"\n",
    "                display_size = details[5].text if len(details) > 5 else \"-\"\n",
    "                battery_capacity = details[6].text if len(details) > 6 else \"-\"\n",
    "\n",
    "                smartphone_info = {\n",
    "                    \"Brand Name\": brand_name,\n",
    "                    \"Smartphone Name\": smartphone_name,\n",
    "                    \"Colour\": color,\n",
    "                    \"RAM\": ram,\n",
    "                    \"Storage(ROM)\": rom,\n",
    "                    \"Primary Camera\": primary_camera,\n",
    "                    \"Secondary Camera\": secondary_camera,\n",
    "                    \"Display Size\": display_size,\n",
    "                    \"Battery Capacity\": battery_capacity,\n",
    "                    \"Price\": price,\n",
    "                    \"Product URL\": product_url\n",
    "                }\n",
    "\n",
    "                smartphone_data.append(smartphone_info)\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "\n",
    "        df = pd.DataFrame(smartphone_data)\n",
    "        df.to_csv(f\"{product}_details.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    scrape_flipkart_smartphones(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceac9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-5\n",
    "import requests\n",
    "\n",
    "def get_coordinates(city):\n",
    "    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"\n",
    "    api_key = \"YOUR_API_KEY\"  # Replace with your Google Maps API key\n",
    "    params = {\n",
    "        \"address\": city,\n",
    "        \"key\": api_key\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['status'] == 'OK':\n",
    "            location = data['results'][0]['geometry']['location']\n",
    "            latitude = location['lat']\n",
    "            longitude = location['lng']\n",
    "            print(f\"Coordinates for {city}: Latitude {latitude}, Longitude {longitude}\")\n",
    "        else:\n",
    "            print(\"Failed to fetch coordinates.\")\n",
    "    else:\n",
    "        print(\"Failed to connect to the API.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_input = input(\"Enter the city to get its coordinates: \")\n",
    "    get_coordinates(city_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26279797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_in():\n",
    "    base_url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops = soup.find_all('div', {'class': 'TopNumbeHeading active'})\n",
    "        laptop_data = []\n",
    "\n",
    "        for laptop in laptops:\n",
    "            details = laptop.find_next('div', {'class': 'Right_TopBlock'})\n",
    "\n",
    "            laptop_name = details.find('h3').text.strip()\n",
    "            specs = details.find_all('p')\n",
    "            specs_dict = {}\n",
    "            for spec in specs:\n",
    "                spec_text = spec.text.strip().split(\":\")\n",
    "                if len(spec_text) == 2:\n",
    "                    specs_dict[spec_text[0]] = spec_text[1]\n",
    "                else:\n",
    "                    specs_dict[spec_text[0]] = \"-\"\n",
    "\n",
    "            laptop_data.append({\n",
    "                \"Laptop Name\": laptop_name,\n",
    "                \"Processor\": specs_dict.get(\"Processor\", \"-\"),\n",
    "                \"OS\": specs_dict.get(\"OS\", \"-\"),\n",
    "                \"Display\": specs_dict.get(\"Display\", \"-\"),\n",
    "                \"Memory\": specs_dict.get(\"Memory\", \"-\"),\n",
    "                \"Weight\": specs_dict.get(\"Weight\", \"-\"),\n",
    "                \"Price\": specs_dict.get(\"Price\", \"-\")\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(laptop_data)\n",
    "        df.to_csv(\"gaming_laptops_digit_in.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_digit_in()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4131107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    base_url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        billionaires = soup.find_all('div', {'class': 'personList'})\n",
    "\n",
    "        billionaires_data = []\n",
    "        for billionaire in billionaires:\n",
    "            rank = billionaire.find('div', {'class': 'rank'}).text.strip()\n",
    "            name = billionaire.find('div', {'class': 'personName'}).text.strip()\n",
    "            net_worth = billionaire.find('div', {'class': 'netWorth'}).text.strip()\n",
    "            age = billionaire.find('div', {'class': 'age'}).text.strip()\n",
    "            citizenship = billionaire.find('div', {'class': 'countryOfCitizenship'}).text.strip()\n",
    "            source = billionaire.find('div', {'class': 'source-column'}).text.strip()\n",
    "            industry = billionaire.find('div', {'class': 'category'}).text.strip()\n",
    "\n",
    "            billionaires_data.append({\n",
    "                \"Rank\": rank,\n",
    "                \"Name\": name,\n",
    "                \"Net Worth\": net_worth,\n",
    "                \"Age\": age,\n",
    "                \"Citizenship\": citizenship,\n",
    "                \"Source\": source,\n",
    "                \"Industry\": industry\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(billionaires_data)\n",
    "        df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-8\n",
    "pip install google-api-python-client\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "\n",
    "DEVELOPER_KEY = 'YOUR_API_KEY'\n",
    "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "YOUTUBE_API_VERSION = 'v3'\n",
    "\n",
    "def get_video_comments(video_id, max_results=500):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    comments = []\n",
    "\n",
    "    results = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=video_id,\n",
    "        maxResults=max_results\n",
    "    ).execute()\n",
    "\n",
    "    for item in results[\"items\"]:\n",
    "        comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "        comment_text = comment[\"textDisplay\"]\n",
    "        like_count = comment.get(\"likeCount\", 0)\n",
    "        published_time = comment[\"publishedAt\"]\n",
    "\n",
    "        comments.append({\n",
    "            \"Comment\": comment_text,\n",
    "            \"Upvotes\": like_count,\n",
    "            \"Published Time\": published_time\n",
    "        })\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Provide a YouTube video ID\n",
    "    video_id = 'YOUR_VIDEO_ID'\n",
    "    video_comments = get_video_comments(video_id)\n",
    "\n",
    "    for comment in video_comments:\n",
    "        print(f\"Comment: {comment['Comment']}\")\n",
    "        print(f\"Upvotes: {comment['Upvotes']}\")\n",
    "        print(f\"Published Time: {comment['Published Time']}\")\n",
    "        print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d8feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-9\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    base_url = \"https://www.hostelworld.com/s/1/London/England\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        hostels = soup.find_all('div', {'class': 'hwta-property-list'})\n",
    "        hostel_data = []\n",
    "\n",
    "        for hostel in hostels:\n",
    "            hostel_name = hostel.find('h2', {'class': 'title'}).text.strip()\n",
    "            distance = hostel.find('span', {'class': 'description'}).text.strip()\n",
    "            ratings = hostel.find('div', {'class': 'score orange'}).text.strip()\n",
    "            total_reviews = hostel.find('div', {'class': 'reviews'}).text.strip()\n",
    "            overall_reviews = hostel.find('div', {'class': 'keyword'}).text.strip()\n",
    "            price_privates = hostel.find('div', {'class': 'price privates from'}).text.strip()\n",
    "            price_dorms = hostel.find('div', {'class': 'price dorms from'}).text.strip()\n",
    "            facilities = \", \".join([item.text.strip() for item in hostel.find_all('li', {'class': 'facility-badge'})])\n",
    "            property_description = hostel.find('div', {'class': 'additional-info'}).text.strip()\n",
    "\n",
    "            hostel_data.append({\n",
    "                \"Hostel Name\": hostel_name,\n",
    "                \"Distance from City Centre\": distance,\n",
    "                \"Ratings\": ratings,\n",
    "                \"Total Reviews\": total_reviews,\n",
    "                \"Overall Reviews\": overall_reviews,\n",
    "                \"Privates From Price\": price_privates,\n",
    "                \"Dorms From Price\": price_dorms,\n",
    "                \"Facilities\": facilities,\n",
    "                \"Property Description\": property_description\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(hostel_data)\n",
    "        df.to_csv(\"hostels_in_london.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hostels_in_london()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f626c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
