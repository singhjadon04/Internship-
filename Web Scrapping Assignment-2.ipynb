{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5d7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q-1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Fill in the search fields and submit the form\n",
    "search_data = {\n",
    "    \"search\": \"Data Analyst\",\n",
    "    \"loc\": \"Bangalore\",\n",
    "}\n",
    "search_url = \"https://www.shine.com/job-search/result/?\"\n",
    "\n",
    "response = requests.get(search_url, params=search_data)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve search results.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Scraping job data for the first 10 results\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "job_data = []\n",
    "results = soup.find_all('li', class_='sjsj')\n",
    "for result in results[:10]:  # Limit to the first 10 results\n",
    "    job_title = result.find('a', class_='job_title').text.strip()\n",
    "    job_location = result.find('span', class_='loc').text.strip()\n",
    "    company_name = result.find('a', class_='job_tittle').find('b').text.strip()\n",
    "    experience_required = result.find('span', class_='exp').text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required,\n",
    "    })\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15c6c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q-2\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Fill in the search fields and submit the form\n",
    "search_data = {\n",
    "    \"search\": \"Data Scientist\",\n",
    "    \"loc\": \"Bangalore\",\n",
    "}\n",
    "search_url = \"https://www.shine.com/job-search/result/?\"\n",
    "\n",
    "response = requests.get(search_url, params=search_data)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve search results.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Scraping job data for the first 10 results\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "job_data = []\n",
    "results = soup.find_all('li', class_='sjsj')\n",
    "for result in results[:10]:  # Limit to the first 10 results\n",
    "    job_title = result.find('a', class_='job_title').text.strip()\n",
    "    job_location = result.find('span', class_='loc').text.strip()\n",
    "    company_name = result.find('a', class_='job_tittle').find('b').text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name,\n",
    "    })\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de73f52c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Apply location filter (Delhi/NCR)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m location_filter \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelhi/NCR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m location_filter_input \u001b[38;5;241m=\u001b[39m \u001b[43mlocation_filter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m location_filter_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchecked\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchecked\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Apply salary filter (3-6 lakhs)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#Q-3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: Fill in the search fields and submit the form\n",
    "search_data = {\n",
    "    \"search\": \"Data Scientist\",\n",
    "}\n",
    "search_url = \"https://www.shine.com/job-search/result/?\"\n",
    "\n",
    "response = requests.get(search_url, params=search_data)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve search results.\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Apply location and salary filters\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Apply location filter (Delhi/NCR)\n",
    "location_filter = soup.find('label', string='Delhi/NCR')\n",
    "location_filter_input = location_filter.find('input')\n",
    "location_filter_input['checked'] = 'checked'\n",
    "\n",
    "# Apply salary filter (3-6 lakhs)\n",
    "salary_filter = soup.find('label', string='3-6 Lakhs')\n",
    "salary_filter_input = salary_filter.find('input')\n",
    "salary_filter_input['checked'] = 'checked'\n",
    "\n",
    "# Update the page with the applied filters\n",
    "response = requests.get(search_url, params=search_data)\n",
    "\n",
    "# Step 4: Scraping job data for the first 10 results\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "job_data = []\n",
    "results = soup.find_all('li', class_='sjsj')\n",
    "for result in results[:10]:  # Limit to the first 10 results\n",
    "    job_title = result.find('a', class_='job_title').text.strip()\n",
    "    job_location = result.find('span', class_='loc').text.strip()\n",
    "    company_name = result.find('a', class_='job_tittle').find('b').text.strip()\n",
    "    experience_required = result.find('span', class_='exp').text.strip()\n",
    "\n",
    "    job_data.append({\n",
    "        'Job Title': job_title,\n",
    "        'Job Location': job_location,\n",
    "        'Company Name': company_name,\n",
    "        'Experience Required': experience_required,\n",
    "    })\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "df = pd.DataFrame(job_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ab459d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1589833190.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [10]\u001b[1;36m\u001b[0m\n\u001b[1;33m    listings = soup.find_all('div', class='_1AtVbE')\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q-4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# URL of the Flipkart sunglasses search results page\n",
    "url = \"https://www.flipkart.com/search?q=sunglasses\"\n",
    "\n",
    "# Loop to scrape data from multiple pages\n",
    "page_count = 0\n",
    "while len(brands) < 100:\n",
    "    # Send a GET request to the current page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the product listings\n",
    "    listings = soup.find_all('div', class='_1AtVbE')\n",
    "\n",
    "    for listing in listings:\n",
    "        # Extract brand, description, and price\n",
    "        brand = listing.find('div', class='_2WkVRV').text\n",
    "        description = listing.find('a', class='IRpwTa').text\n",
    "        price = listing.find('div', class='_30jeq3').text\n",
    "\n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "    # Find the \"Next\" button and get the next page URL\n",
    "    next_button = soup.find('a', class='_1LKTO3')\n",
    "\n",
    "    if next_button:\n",
    "        url = \"https://www.flipkart.com\" + next_button['href']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Brand': brands[:100],\n",
    "    'Product Description': descriptions[:100],\n",
    "    'Price': prices[:100],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69f81790",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2693144086.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [13]\u001b[1;36m\u001b[0m\n\u001b[1;33m    listings = soup.find_all('div', class='_1AtVbE')\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q-5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# URL of the Flipkart iPhone 11 reviews page\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Loop to scrape data from multiple pages\n",
    "page_count = 0\n",
    "while len(ratings) < 100:\n",
    "    # Send a GET request to the current page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the review listings\n",
    "    listings = soup.find_all('div', class='_1AtVbE')\n",
    "\n",
    "    for listing in listings:\n",
    "        # Extract rating, review summary, and full review\n",
    "        rating = listing.find('div', class='_3LWZlK').text\n",
    "        review_summary = listing.find('p', class='_2-N8zT').text\n",
    "        full_review = listing.find('div', class='_1bBC2W').text\n",
    "\n",
    "        ratings.append(rating)\n",
    "        review_summaries.append(review_summary)\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "    # Find the \"Next\" button and get the next page URL\n",
    "    next_button = soup.find('a', class='_1LKTO3')\n",
    "\n",
    "    if next_button:\n",
    "        url = \"https://www.flipkart.com\" + next_button['href']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Rating': ratings[:100],\n",
    "    'Review Summary': review_summaries[:100],\n",
    "    'Full Review': full_reviews[:100],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1836b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (611173127.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [14]\u001b[1;36m\u001b[0m\n\u001b[1;33m    listings = soup.find_all('div', class='_1AtVbE')\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q-6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# URL of the Flipkart sneakers search results page\n",
    "url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "\n",
    "# Loop to scrape data from multiple pages\n",
    "page_count = 0\n",
    "while len(brands) < 100:\n",
    "    # Send a GET request to the current page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the product listings\n",
    "    listings = soup.find_all('div', class='_1AtVbE')\n",
    "\n",
    "    for listing in listings:\n",
    "        # Extract brand, description, and price\n",
    "        brand = listing.find('div', class='_2WkVRV').text\n",
    "        description = listing.find('a', class='IRpwTa').text\n",
    "        price = listing.find('div', class='_30jeq3').text\n",
    "\n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "    # Find the \"Next\" button and get the next page URL\n",
    "    next_button = soup.find('a', class='_1LKTO3')\n",
    "\n",
    "    if next_button:\n",
    "        url = \"https://www.flipkart.com\" + next_button['href']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Brand': brands[:100],\n",
    "    'Product Description': descriptions[:100],\n",
    "    'Price': prices[:100],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c648cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page.\n",
      "Empty DataFrame\n",
      "Columns: [Title, Ratings, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q-7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# URL of the Amazon laptops search results page with CPU Type filter applied\n",
    "url = \"https://www.amazon.in/s?k=Laptop&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031&dc&qid=1672133086&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_8\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the product listings\n",
    "listings = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "# Loop to scrape data for the first 10 laptops\n",
    "for listing in listings[:10]:\n",
    "    # Extract title\n",
    "    title = listing.find('span', class_='a-text-normal').text.strip()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Extract ratings (if available)\n",
    "    rating_element = listing.find('span', class_='a-icon-alt')\n",
    "    if rating_element:\n",
    "        rating = rating_element.text\n",
    "    else:\n",
    "        rating = 'N/A'\n",
    "    ratings.append(rating)\n",
    "\n",
    "    # Extract price (if available)\n",
    "    price_element = listing.find('span', class_='a-price-whole')\n",
    "    if price_element:\n",
    "        price = price_element.text\n",
    "    else:\n",
    "        price = 'N/A'\n",
    "    prices.append(price)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Title': titles,\n",
    "    'Ratings': ratings,\n",
    "    'Price': prices,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b2a479",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (579603182.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    quote_items = soup.find_all('div', class='title')\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Q-8\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "quotes = []\n",
    "authors = []\n",
    "quote_types = []\n",
    "\n",
    "# URL of the AzQuotes website\n",
    "url = \"https://www.azquotes.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and click on the \"Top Quotes\" link\n",
    "top_quotes_link = soup.find('a', text='Top Quotes')\n",
    "if top_quotes_link:\n",
    "    top_quotes_url = \"https://www.azquotes.com\" + top_quotes_link['href']\n",
    "\n",
    "    response = requests.get(top_quotes_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the Top Quotes page.\")\n",
    "        exit()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and scrape data for the Top 1000 Quotes of All Time\n",
    "quote_items = soup.find_all('div', class='title')\n",
    "\n",
    "for item in quote_items:\n",
    "    # Extract quote\n",
    "    quote = item.find('a', class='titleText').text.strip()\n",
    "    quotes.append(quote)\n",
    "\n",
    "    # Extract author\n",
    "    author = item.find('a', class='authorOrTitle').text.strip()\n",
    "    authors.append(author)\n",
    "\n",
    "    # Extract type of quote\n",
    "    quote_type = item.find('div', class='p-q-cont').text.strip()\n",
    "    quote_types.append(quote_type)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Quote': quotes,\n",
    "    'Author': authors,\n",
    "    'Type Of Quote': quote_types,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-9\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the web page\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the list of Prime Ministers\n",
    "table = soup.find('table', class_='MsoNormalTable')\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "born_dead = []\n",
    "term_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Loop through table rows and extract data\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "\n",
    "    name = columns[0].text.strip()\n",
    "    names.append(name)\n",
    "\n",
    "    bd = columns[1].text.strip()\n",
    "    born_dead.append(bd)\n",
    "\n",
    "    term = columns[2].text.strip()\n",
    "    term_of_office.append(term)\n",
    "\n",
    "    remark = columns[3].text.strip()\n",
    "    remarks.append(remark)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': term_of_office,\n",
    "    'Remarks': remarks,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e979ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Car Name, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Q-10\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "# URL of the Motor1 website\n",
    "url = \"https://www.motor1.com/\"\n",
    "\n",
    "# Send a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and click on the search bar\n",
    "search_bar = soup.find('input', id='q')\n",
    "\n",
    "if search_bar:\n",
    "    # Type in '50 most expensive cars'\n",
    "    search_bar['value'] = '50 most expensive cars'\n",
    "\n",
    "    # Find and click on the search button\n",
    "    search_button = soup.find('button', class_='search-button')\n",
    "\n",
    "    if search_button:\n",
    "        search_button.click()\n",
    "\n",
    "        # Wait for the search results page to load\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(\"Failed to retrieve the search results page.\")\n",
    "            exit()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find and click on the link '50 most expensive cars in the world'\n",
    "        link = soup.find('a', text='50 most expensive cars in the world')\n",
    "\n",
    "        if link:\n",
    "            link_url = link['href']\n",
    "\n",
    "            response = requests.get(link_url)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(\"Failed to retrieve the page with the list of expensive cars.\")\n",
    "                exit()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the list of expensive cars\n",
    "            car_list = soup.find('div', class_='vlp-slideshow vlp-slideshow-article vlp-car-slideshow')\n",
    "\n",
    "            if car_list:\n",
    "                car_items = car_list.find_all('div', class_='vlp-slide')\n",
    "\n",
    "                for car in car_items:\n",
    "                    # Extract car name\n",
    "                    car_name = car.find('h2', class_='vlp-slide-title').text.strip()\n",
    "                    car_names.append(car_name)\n",
    "\n",
    "                    # Extract car price\n",
    "                    car_price = car.find('span', class_='vlp-price').text.strip()\n",
    "                    car_prices.append(car_price)\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "data = {\n",
    "    'Car Name': car_names,\n",
    "    'Price': car_prices,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047261e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
