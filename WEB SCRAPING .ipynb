{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab22c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Text\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#Q-1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Wikipedia page you want to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all header tags (h1, h2, h3, h4, h5, h6) on the page\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # Extract the text from the header tags\n",
    "    header_texts = [header.text for header in header_tags]\n",
    "\n",
    "    # Create a DataFrame from the header texts\n",
    "    df = pd.DataFrame({'Header Text': header_texts})\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fabef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page.\n"
     ]
    }
   ],
   "source": [
    "#Q-2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page with the list of former presidents\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the list of former presidents\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    names = []\n",
    "    terms_of_office = []\n",
    "\n",
    "    # Iterate through the rows of the table\n",
    "    for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 2:\n",
    "            name = columns[0].text.strip()\n",
    "            term = columns[1].text.strip()\n",
    "            names.append(name)\n",
    "            terms_of_office.append(term)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms_of_office})\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac0b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Men's Cricket:\n",
      "  Position              Team Matches Points Rating\n",
      "0        1        India\\nIND      49  5,839    119\n",
      "1        2    Australia\\nAUS      36  4,015    112\n",
      "2        3     Pakistan\\nPAK      32  3,525    110\n",
      "3        4  South Africa\\nSA      29  3,166    109\n",
      "4        5   New Zealand\\nNZ      38  4,007    105\n",
      "5        6      England\\nENG      34  3,377     99\n",
      "6        7     Sri Lanka\\nSL      43  3,943     92\n",
      "7        8   Bangladesh\\nBAN      40  3,574     89\n",
      "8        9  Afghanistan\\nAFG      26  2,170     83\n",
      "9       10   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 5 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 4 columns passed, passed data had 5 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m batsmen_headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     48\u001b[0m batsmen_data \u001b[38;5;241m=\u001b[39m batsmen_table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]  \u001b[38;5;66;03m# Top 10 batsmen\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m odi_batsmen_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatsmen_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatsmen_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 ODI Batsmen:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(odi_batsmen_df)\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mcreate_dataframe\u001b[1;34m(data, headers)\u001b[0m\n\u001b[0;32m     20\u001b[0m     record \u001b[38;5;241m=\u001b[39m [column\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns]\n\u001b[0;32m     21\u001b[0m     records\u001b[38;5;241m.\u001b[39mappend(record)\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;66;03m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;66;03m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m         arrays,\n\u001b[0;32m    731\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 519\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    880\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    881\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 883\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    988\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 4 columns passed, passed data had 5 columns"
     ]
    }
   ],
   "source": [
    "#Q-3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from ICC website\n",
    "def scrape_icc_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract data and create a DataFrame\n",
    "def create_dataframe(data, headers):\n",
    "    records = []\n",
    "\n",
    "    for row in data:\n",
    "        columns = row.find_all('td')\n",
    "        record = [column.get_text().strip() for column in columns]\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=headers)\n",
    "    return df\n",
    "\n",
    "# URL for ODI Team Rankings\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "\n",
    "odi_teams_soup = scrape_icc_data(odi_teams_url)\n",
    "\n",
    "if odi_teams_soup:\n",
    "    teams_table = odi_teams_soup.find(\"table\", class_=\"table\")\n",
    "    team_headers = [\"Position\", \"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "    team_data = teams_table.find_all(\"tr\")[1:11]  # Top 10 teams\n",
    "    odi_teams_df = create_dataframe(team_data, team_headers)\n",
    "    print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "    print(odi_teams_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# URL for ODI Batsmen Rankings\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "\n",
    "odi_batsmen_soup = scrape_icc_data(odi_batsmen_url)\n",
    "\n",
    "if odi_batsmen_soup:\n",
    "    batsmen_table = odi_batsmen_soup.find(\"table\", class_=\"table\")\n",
    "    batsmen_headers = [\"Position\", \"Player\", \"Team\", \"Rating\"]\n",
    "    batsmen_data = batsmen_table.find_all(\"tr\")[1:11]  # Top 10 batsmen\n",
    "    odi_batsmen_df = create_dataframe(batsmen_data, batsmen_headers)\n",
    "    print(\"Top 10 ODI Batsmen:\")\n",
    "    print(odi_batsmen_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# URL for ODI Bowlers Rankings\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "odi_bowlers_soup = scrape_icc_data(odi_bowlers_url)\n",
    "\n",
    "if odi_bowlers_soup:\n",
    "    bowlers_table = odi_bowlers_soup.find(\"table\", class_=\"table\")\n",
    "    bowlers_headers = [\"Position\", \"Player\", \"Team\", \"Rating\"]\n",
    "    bowlers_data = bowlers_table.find_all(\"tr\")[1:11]  # Top 10 bowlers\n",
    "    odi_bowlers_df = create_dataframe(bowlers_data, bowlers_headers)\n",
    "    print(\"Top 10 ODI Bowlers:\")\n",
    "    print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe823f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams:\n",
      "  Position              Team Matches Points Rating\n",
      "0        1    Australia\\nAUS      19  3,084    162\n",
      "1        2      England\\nENG      23  2,991    130\n",
      "2        3  South Africa\\nSA      21  2,446    116\n",
      "3        4        India\\nIND      18  1,745     97\n",
      "4        5   New Zealand\\nNZ      21  2,014     96\n",
      "5        6   West Indies\\nWI      18  1,610     89\n",
      "6        7     Sri Lanka\\nSL       9    714     79\n",
      "7        8   Bangladesh\\nBAN      11    816     74\n",
      "8        9     Thailand\\nTHA      11    753     68\n",
      "9       10     Pakistan\\nPAK      21  1,435     68\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 5 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:982\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1030\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1032\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m     )\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 4 columns passed, passed data had 5 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m batting_headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     48\u001b[0m batting_data \u001b[38;5;241m=\u001b[39m batting_table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m]  \u001b[38;5;66;03m# Top 10 batsmen\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m women_odi_batting_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatting_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatting_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 Women\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ODI Batting Players:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(women_odi_batting_df)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mcreate_dataframe\u001b[1;34m(data, headers)\u001b[0m\n\u001b[0;32m     20\u001b[0m     record \u001b[38;5;241m=\u001b[39m [column\u001b[38;5;241m.\u001b[39mget_text()\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m columns]\n\u001b[0;32m     21\u001b[0m     records\u001b[38;5;241m.\u001b[39mappend(record)\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:721\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"ensure_index\" has incompatible type\u001b[39;00m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;66;03m# \"Collection[Any]\"; expected \"Union[Union[Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;66;03m# ndarray], Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[0;32m    720\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    730\u001b[0m         arrays,\n\u001b[0;32m    731\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    734\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    735\u001b[0m     )\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:519\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    517\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 519\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:883\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    880\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    881\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 883\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:985\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    982\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 985\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    988\u001b[0m     contents \u001b[38;5;241m=\u001b[39m _convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 4 columns passed, passed data had 5 columns"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from ICC website\n",
    "def scrape_icc_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract data and create a DataFrame\n",
    "def create_dataframe(data, headers):\n",
    "    records = []\n",
    "\n",
    "    for row in data:\n",
    "        columns = row.find_all('td')\n",
    "        record = [column.get_text().strip() for column in columns]\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=headers)\n",
    "    return df\n",
    "\n",
    "# URL for Women's ODI Team Rankings\n",
    "women_odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "women_odi_teams_soup = scrape_icc_data(women_odi_teams_url)\n",
    "\n",
    "if women_odi_teams_soup:\n",
    "    teams_table = women_odi_teams_soup.find(\"table\", class_=\"table\")\n",
    "    team_headers = [\"Position\", \"Team\", \"Matches\", \"Points\", \"Rating\"]\n",
    "    team_data = teams_table.find_all(\"tr\")[1:11]  # Top 10 teams\n",
    "    women_odi_teams_df = create_dataframe(team_data, team_headers)\n",
    "    print(\"Top 10 Women's ODI Teams:\")\n",
    "    print(women_odi_teams_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# URL for Women's ODI Batting Players Rankings\n",
    "women_odi_batting_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "\n",
    "women_odi_batting_soup = scrape_icc_data(women_odi_batting_url)\n",
    "\n",
    "if women_odi_batting_soup:\n",
    "    batting_table = women_odi_batting_soup.find(\"table\", class_=\"table\")\n",
    "    batting_headers = [\"Position\", \"Player\", \"Team\", \"Rating\"]\n",
    "    batting_data = batting_table.find_all(\"tr\")[1:11]  # Top 10 batsmen\n",
    "    women_odi_batting_df = create_dataframe(batting_data, batting_headers)\n",
    "    print(\"Top 10 Women's ODI Batting Players:\")\n",
    "    print(women_odi_batting_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# URL for Women's ODI All-Rounder Rankings\n",
    "women_odi_allrounder_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "women_odi_allrounder_soup = scrape_icc_data(women_odi_allrounder_url)\n",
    "\n",
    "if women_odi_allrounder_soup:\n",
    "    allrounder_table = women_odi_allrounder_soup.find(\"table\", class_=\"table\")\n",
    "    allrounder_headers = [\"Position\", \"Player\", \"Team\", \"Rating\"]\n",
    "    allrounder_data = allrounder_table.find_all(\"tr\")[1:11]  # Top 10 all-rounders\n",
    "    women_odi_allrounder_df = create_dataframe(allrounder_data, allrounder_headers)\n",
    "    print(\"Top 10 Women's ODI All-Rounders:\")\n",
    "    print(women_odi_allrounder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c1b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNBC World News:\n",
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from CNBC website\n",
    "def scrape_cnbc_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract data and create a DataFrame\n",
    "def create_dataframe(data, headers):\n",
    "    records = []\n",
    "\n",
    "    for item in data:\n",
    "        headline = item.find(\"h3\", class_=\"Card-title\")\n",
    "        time = item.find(\"time\", class_=\"Card-timestamp\")\n",
    "        link = item.find(\"a\", class_=\"Card-titleLink\")\n",
    "        \n",
    "        if headline and time and link:\n",
    "            headline_text = headline.get_text().strip()\n",
    "            time_text = time.get_text().strip()\n",
    "            news_link = link['href']\n",
    "            \n",
    "            records.append([headline_text, time_text, news_link])\n",
    "\n",
    "    df = pd.DataFrame(records, columns=headers)\n",
    "    return df\n",
    "\n",
    "# URL for CNBC World news\n",
    "cnbc_world_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "cnbc_world_soup = scrape_cnbc_data(cnbc_world_url)\n",
    "\n",
    "if cnbc_world_soup:\n",
    "    news_items = cnbc_world_soup.find_all(\"div\", class_=\"Card\")\n",
    "    news_headers = [\"Headline\", \"Time\", \"News Link\"]\n",
    "    cnbc_world_df = create_dataframe(news_items, news_headers)\n",
    "    print(\"CNBC World News:\")\n",
    "    print(cnbc_world_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfa0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles in AI (Last 90 Days):\n",
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from Elsevier website\n",
    "def scrape_elsevier_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract data and create a DataFrame\n",
    "def create_dataframe(data, headers):\n",
    "    records = []\n",
    "\n",
    "    for item in data:\n",
    "        title = item.find(\"a\", class_=\"anchor js-create-citation\")\n",
    "        authors = item.find(\"div\", class_=\"text-s authors-list\")\n",
    "        date = item.find(\"div\", class_=\"text-s published-date\")\n",
    "        paper_url = item.find(\"a\", class_=\"anchor js-create-citation\")['href']\n",
    "        \n",
    "        if title and authors and date and paper_url:\n",
    "            title_text = title.get_text().strip()\n",
    "            authors_text = authors.get_text().strip()\n",
    "            date_text = date.get_text().strip()\n",
    "            \n",
    "            records.append([title_text, authors_text, date_text, paper_url])\n",
    "\n",
    "    df = pd.DataFrame(records, columns=headers)\n",
    "    return df\n",
    "\n",
    "# URL for most downloaded articles in AI from Elsevier\n",
    "elsevier_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "elsevier_soup = scrape_elsevier_data(elsevier_url)\n",
    "\n",
    "if elsevier_soup:\n",
    "    article_items = elsevier_soup.find_all(\"li\", class_=\"pod-listing\")\n",
    "    article_headers = [\"Paper Title\", \"Authors\", \"Published Date\", \"Paper URL\"]\n",
    "    elsevier_df = create_dataframe(article_items, article_headers)\n",
    "    print(\"Most Downloaded Articles in AI (Last 90 Days):\")\n",
    "    print(elsevier_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b034a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape data from the target website\n",
    "def scrape_data(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Function to extract the desired data and create a data structure (e.g., list of dictionaries)\n",
    "def extract_data(soup):\n",
    "    restaurant_data = []\n",
    "\n",
    "    # Find and loop through the containers that hold restaurant information\n",
    "    restaurant_containers = soup.find_all(\"div\", class_=\"restaurant-container\")\n",
    "\n",
    "    for container in restaurant_containers:\n",
    "        restaurant = {}\n",
    "\n",
    "        # Extract restaurant name\n",
    "        restaurant['Name'] = container.find(\"h2\", class_=\"restaurant-name\").text.strip()\n",
    "\n",
    "        # Extract cuisine type\n",
    "        restaurant['Cuisine'] = container.find(\"p\", class_=\"cuisine-type\").text.strip()\n",
    "\n",
    "        # Extract location\n",
    "        restaurant['Location'] = container.find(\"p\", class_=\"restaurant-location\").text.strip()\n",
    "\n",
    "        # Extract ratings\n",
    "        restaurant['Ratings'] = container.find(\"span\", class_=\"restaurant-ratings\").text.strip()\n",
    "\n",
    "        restaurant_data.append(restaurant)\n",
    "\n",
    "    return restaurant_data\n",
    "\n",
    "# URL of the website page you want to scrape\n",
    "url = \"https://example.com/restaurants\"\n",
    "\n",
    "# Scrape the data\n",
    "soup = scrape_data(url)\n",
    "\n",
    "if soup:\n",
    "    # Extract the data\n",
    "    data = extract_data(soup)\n",
    "\n",
    "    # Create a DataFrame or print the data as needed\n",
    "    # Example: df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20080abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f5467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91c0107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bde292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a3e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088717f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
